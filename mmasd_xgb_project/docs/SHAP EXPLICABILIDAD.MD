

# üìñ Documentaci√≥n de referencia: SHAP en el MVP

## 1. ¬øQu√© es SHAP?

**SHAP (SHapley Additive exPlanations)** es una **librer√≠a de Python** para explicar modelos de Machine Learning.
No es un modelo predictivo, sino una herramienta de **explicabilidad**.

Se basa en la **teor√≠a de valores de Shapley** (teor√≠a de juegos cooperativos), que asigna a cada **feature/variable** una contribuci√≥n al resultado final de una predicci√≥n.

üëâ **Pregunta que responde SHAP:**
**‚Äú¬øPor qu√© el modelo predijo este resultado?‚Äù**

---

## 2. ¬øC√≥mo funciona?

* Cada predicci√≥n del modelo se descompone en las contribuciones de cada variable.
* La suma de todas esas contribuciones = predicci√≥n final.

Ejemplo simple:

* Predicci√≥n de riesgo: **0.82**
* Variables que influyen:

  * `asistencia_clases baja` (+0.25 riesgo)
  * `pocos_horas_sue√±o` (+0.12 riesgo)
  * `puntaje_test_memoria alto` (‚àí0.05 riesgo)
  * `intervenciones_previas` (‚àí0.02 riesgo)

---

## 3. Ejemplo en c√≥digo (con XGBoost)

```python
import shap
import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split

# Dataset proxy (ejemplo con MMASD adaptado)
df = pd.read_csv("data/processed/mmasd_features.csv")
X = df.drop(columns=["target"])
y = df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenamos modelo baseline
model = xgb.XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05)
model.fit(X_train, y_train)

# Creamos el objeto SHAP
explainer = shap.TreeExplainer(model)

# Calculamos valores SHAP
shap_values = explainer.shap_values(X_test)

# --- Visualizaci√≥n global ---
shap.summary_plot(shap_values, X_test)

# --- Visualizaci√≥n local (ejemplo de un ni√±o espec√≠fico) ---
i = 0  # √≠ndice del primer caso en test
shap.force_plot(explainer.expected_value, shap_values[i,:], X_test.iloc[i,:])
```

---

## 4. Tipos de visualizaci√≥n

* **Global (summary plot):**
  Ordena variables por importancia en todo el dataset.
  Ejemplo: `asistencia_clases` > `puntaje_memoria` > `horas_sue√±o`.

* **Local (force plot):**
  Explica una predicci√≥n individual.
  Ejemplo:

  * La baja asistencia **empuja hacia mayor riesgo**.
  * La intervenci√≥n terap√©utica **empuja hacia menor riesgo**.

---

## 5. Importancia para el proyecto

1. **Transparencia:** evita la ‚Äúcaja negra‚Äù del modelo.
2. **√âtica:** explicaciones claras, no diagn√≥sticos autom√°ticos.
3. **Roles diferenciados:**

   * **Familia:** ‚ÄúEl riesgo aument√≥ porque la asistencia baj√≥‚Äù.
   * **Docente:** ‚ÄúEl faltante de tareas fue clave en la predicci√≥n‚Äù.
   * **Terapeuta:** ‚ÄúLos √∫ltimos puntajes cl√≠nicos redujeron el riesgo‚Äù.

---

## 6. Conexi√≥n con el MVP

* Se aplicar√° **SHAP a los modelos entrenados (XGBoost)** para generar explicaciones:

  * **Locales** (para cada ni√±o/ni√±a).
  * **Globales** (para todo el dataset).
* Los resultados se mostrar√°n en **dashboards por rol** con lenguaje adaptado:

  * **Familia:** mensajes simples y claros.
  * **Docentes/Terapeutas:** gr√°ficos con top-variables SHAP.

---

‚úÖ En resumen:
**SHAP = librer√≠a de explicabilidad para ML**.
Se integra a tu MVP para dar **explicaciones globales y locales** de las predicciones, reforzando la confianza, √©tica y utilidad del sistema.

---
